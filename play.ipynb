{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00afb48-2780-4a64-bbb2-86e6248e4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9d29e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./chatbot_env/lib/python3.12/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install --upgrade --quiet langchain\n",
    "!pip install --upgrade --quiet langchain-core\n",
    "!pip install --upgrade --quiet langchain-community\n",
    "!pip install --upgrade --quiet langchain-together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252a385",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a4e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_together import ChatTogether\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# create an object of an LLM model using TogetherAI\n",
    "llm = ChatTogether(api_key=os.getenv(\"TOGETHER_API_KEY\"), temperature=0.0, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcff169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube_transcript_api in ./chatbot_env/lib/python3.12/site-packages (0.6.2)\n",
      "Requirement already satisfied: requests in ./chatbot_env/lib/python3.12/site-packages (from youtube_transcript_api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./chatbot_env/lib/python3.12/site-packages (from requests->youtube_transcript_api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./chatbot_env/lib/python3.12/site-packages (from requests->youtube_transcript_api) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./chatbot_env/lib/python3.12/site-packages (from requests->youtube_transcript_api) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./chatbot_env/lib/python3.12/site-packages (from requests->youtube_transcript_api) (2024.8.30)\n",
      "Requirement already satisfied: pytube in ./chatbot_env/lib/python3.12/site-packages (15.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube_transcript_api\n",
    "!pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83f449ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# get data from youtube url using YoutubeLoader\n",
    "video_url = \"https://www.youtube.com/watch?v=5k1zkYCuF-8\"\n",
    "loader = YoutubeLoader.from_youtube_url(video_url, add_video_info = False)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1512d00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"prompt engineering makes complete sense you tell the AI precisely what you want to do in order to squeeze out every last bit of its performance until you start to get some unreasonable improvements by telling the chat Bots to read the question again or simply giving them a tip maybe because they're born and trained in the US and look at that they even made prompt engineering into a real high-paying job and just to talk to chat Bots all day how ridiculous but this seemingly basic practice has been in evolving into something extremely complex and arguably one of the most over engineered aspects of large language modeling like we got all these methods to choose from with some of the latest research still competing against each other about what is the best prompting method but at the end of the day do all these really matter because beneath all these variations they are all ultimately focused on enhancing the autor regressive process within llm that is the process of predicting the next token in a sequence based on all the tokens that came before this includes every word you have entered and every token the AI has ever generated and it is this process that explains why F shot prompting Chain of Thought prompt engineering and even read the question again actually works because the idea is the more words there are the more information the AI can use as reference but it's not really that simple at the same time when the AI is trying to predict the next word after learning all the textual knowledge the entire internet has to offer however it should already make more sense for you now now that prompting shortly is always a bad idea so in addition to provide enough context for the AI to condition on the prompt also needs to resemble the kind of data the model was trained on and when both of these factors align you would be able to draw out the lm's full potential just like this amazing promp engineering rundown for LM that HubSpot has put together for free from a non-technical perspective the AI for business Builders guide is designed to help you build a strong intuition on how to work with AI in your business and think about AI from the perspective of input and output it provides you a great overview for the key techniques that made AI chatbot so powerful so if any of the stuff I talked about today goes over your head this will be the crash course for you you'll learn how AI truly works behind the scenes how to communicate with large language models effectively and when it will be the time to invest in advanced AI tools for your business this guide also covers what resources AI requires helping you make informed decisions about integrating AI into your workflow and you don't need to be technical to understand it either whether you're generating content automating customer service or analyzing data the guide breaks it all down so you can start leveraging AI immediately my favorite section is prompt engineering with API as what I've mostly talked about on my channel is the theoretical stuff this part of their course really complements with how to use LM in practice it'll introduce you to the basic of apis how to manage it and provide you with example use cases that can give you inspiration it's important to begin implementing AI whether you're growing a business or simply want to better understand how it all works so I highly recommend you to check out the free llm crash course with the link down in the description and thank you helpspot for sponsoring this video in recent news opena has released another language model series called 01 that excels at reasoning and logical tasks with its key breakthrough being using Chain of Thought enabling the model to think for a while before arriving at an insert I can already imagine how traditional media will twist this with their headlines but anyways this video will be a more gentle introduction to prompt engineering and in a future video I'll dive for slowly into Chain of Thought and the new test time compute idea that might be the new paradigm open Ai and Google are currently looking towards I also believe that this Chain of Thought mechanism has already been implemented into CLA 3.5 Sonet natively through its inner thought special token that cla's artifact utilizes this way clot can have planning and self verification all within one operation and keep the model focused on one task without going off the rails as the last thing an AI model is good at is being consistent bad implementation of Chain of Thought would make this fragile process even more fragile because with one wrong step inside the chain of thought process or an incorrect prompt formatting its performance would crash big time and in general if the model isn't used optimally your results will be terrible too so getting Chain of Thought right is extremely important and hard but maybe I'm getting ahead of myself so let me first explain what prompt engineering is what Chain of Thought is how ai's inner thoughts potentially work and how it all might just be a load of BS first of all there's a paper about prompt engineering from two years ago that dives into the details about how these chat Bots already know what they know which basically means that it'll be extremely difficult for it to perform well on something it has never learned about AKA out of distribution data so realistically prompt engineering isn't really some magic that boosts a model's performance but think of it more like squeezing a lemon with a juicer instead of by hand to better extract all the juice it is a way to tap into the capabilities that the model already learned in the pre-training stage and juicing out its full capabilities so if a model was never trained on a guy named bcloud that makes AI slop on YouTube then when you ask the model who by cloud is the model would not know but it might tell you to subscribe this is also called zero shot prompting where you ask the model directly to perform a task without any examples and the model can only work based on its prior knowledge and the prompt you provide so to make it slightly better to use futa prompting is where you give the a few examples to guide its response to do a specific task kind of like giving it a few demos so it can better replicate what you want it to do aside from F shot prompting Chino thought may feel like it's something similar but not really as F shot is providing question answer pairs as examples while Chain of Thought is to have the model articulate intermediate reasoning steps when solving complex problems rather than directly providing a final answer so we can have something called a few shot Chain of Thought where you give examples of how a model can generate Chain of Thought and have the model copy the format similar similarly you can insert more instructions into the chain of thought process problem decomposition is another instruction that you can add on top of Chain of Thought an example of this is plan and solve that would have a model plan on how to solve a question by breaking it down into smaller steps to better approach the problem and if these prompt engineering methods sound familiar it is because they are often used in benchmarks zero shot prompting will usually show what the model is capable of right out of the box F shw prompting demonstrates the model's capabilities to generalize and follow unseen instructions and Chain of Thought tries to get the model to reason more carefully when trying to find the correct answer but why are some Chain of Thought implementations better than others why is that when you use Chain of Thought on a Model your result is still dog water but when claw 3.5 son or open A1 does Chain of Thought it just blows everyone out of the water so there is a huge difference when the model is seeing this Chain of Thought method for the first time compared to being trained on it beforehand I'm a trained professional so so in order to make an AI chatbot there are two main stages the pre-training and the post-training stage and both stages are super important in the process if you're unsure about what exactly pre- and posttraining is you should check out this video but in short pre-training basically learns all the knowledge there is usually by reading the entire internet and in post training it learns to become interactive and provide formatted answers for the user so functions like becoming a chatbot that a user can exchange messages with or using a calculator when a math question is presented these skills are all learned at this posttraining stage to get the model to do all this though there needs to be Flags to Signal which part of the process it's in so to get the model to call a calculator API you would train the model with example dialogues of how the model can use the calculator kind of like few shot prompting and since you need a lot of examples to pick the idea of calling the calculator API when presented with a math question researchers would usually use something called human augmented synthetic data which is just a fancy name to use another llm to generate examples and have humans observe and manually fix the synthetic data and in order to indicate what the L should say to call the calculator you would have to add something called special tokens these special tokens are also used to indicate the end of an answer assistant prompt when it is answering as an assistant Etc so you would need a ton of synthetic data for instance the entire internet with usable knowledge is around 3 trillion tokens while the synthetic data set that llama 3.1 45b used is four times its size which is estimated to be around 12 trillion tokens so what anthropic researchers probably did to implement chain of thought into 3.5 Sonet was to add a new type of special token called end thinking to represent inner dialogue for the model while generating the output for the user this special token has probably been trained and used as a buil-in routine for the model to activate Chain of Thought during its generation process as it should probably help the model to generate more more consistently over the long run as for open eyes 01 their chain of thought process is completely hidden maybe it is to prevent people from reverse engineering it or distilling a model from it so in order to bake in Chain of Thought in cla's case they had to synthesize huge amounts of examples of how to use a special token and thinking during its instruction tuning stage so when model is creating artifacts which is the functions that calls end thinking it can draw out the model's full potential by properly using their version of Chain of Thought that is end thinking as for the reason why you don't see its end thinking tag or its inner dialogue or in general any other special tokens they just simply have their UI front end remove that text before presenting the results to you so that's my theory on why claw 3.5 or open a1's Chain of Thought is so much more effective than your typical Chain of Thought prompt template when used on any other model as llms are highly sensitive to prompt variations where subtle changes like word order spacing and capitalization can drastically impact model performance so just blindly providing more tokens for the model to condition on may not be as ideal because if the model has never learned anything about Chain of Thought or has a different post-training format the results may only be suboptimal and it'll be hard for the model to stay consistent even with the extra information this is also why chat Bots with strict safety guard rails feels like lobotomized as these safety instructions obstruct the model from making good connections with this knowledge space since the question is aisc by many other unrelated safety instructions within its context window on the other hand self-reflection is yet another interesting approach as it introduces self-verification during the generation process allowing the model to iteratively check its answers it's PR known that this can significantly improve performance across reasoning and generation tasks but it's also a pain to use it in practice because they would often still get the answer wrong as what people usually ask are outof distribution questions and even under the hood they would often change their mind on whether an answer is correct or not but anyways if we all take a big step back all these methods like change of thought and reflection are just generating relevant tokens that help condition and predict the next token more accurately right in the latest open ey report on 01 they Pro practically that there's a new scalable aspect of llm that is the more amount of tokens a model generates the better it performs on reasoning this have shown huge improvements in performance on incredibly hard logical tasks but only on those tasks with other non- reasoning related tasks like English literature performance stays stagnant which kind of shows that channel thought related methods are just squeezing the juice out of the lemon like I described earlier where it just helps the model to squeeze out its already built-in capabilities and not a method that will upgrade or boost the model as for the slightly New Concept called test time compute I have a lot to say about that so so I guess I'll cover that in a separate video but is Chain of Thought really that beneficial does the model truly benefit from explaining to itself well in this paper called think before you speak they have found out from their experiments that if an LL were given more tokens to figure things out orbe it padding with meaningless tokens there would still be a reasonable increase in performance the researchers then concluded that by delaying the generation of the final answer the performance will always be better which they link to Chanel of thought as it is technically a method that can delay the final answer and with a more natural process that would contain meaningful tokens that we can also understand so maybe we just need to let the model Yap it out a bit and it might just achieve a comparable effect to Chain of Thought or self-reflection some even say that the models need to doodle on their scratch pad for a little bit before it arrives at an answer so it does feel like these extensive prompting methods may just be an illusion that people mistake for pseudo science because it might be a coincidence that they work on L's and the methodology seems logical to us that said fuse shot prompting is still effective and prompting closely to the training data is the correct way to use it however another research paper called futur lens presents another very interesting perspective onto the table so through a handful of experiments the researchers were able to show that when the model is generating a token the information in the neuron Network that is used to generate that token would also contain enough information to predict the next one or two tokens with an accuracy of up to 48% which means if you're lucky enough enough generating one token would technically give you three tokens in total this paper actually paved a handful of really unique methods for probing the internal workings of language models and understanding what they know about the future sequence at each step so it might be possible to optimize the process and reduce the number of layers needed for generating text and we might have just been brute forcing prompt engineering methods onto llm which may me think that prompt engineering is unreasonable because we still kind of don't really understand the process of Auto aggressive language modeling like on one hand it does feel like train of thought and various extensive prompting methods do work and make sense logically but on the other it feels like it all might be a placebo effect when it's too over complicated like have you seen how many papers that's solely focused on prompt engineering but to be honest the same could be said about how we don't fully understand how Transformers work so at the end of the day when the model doesn't do exactly what you want don't pay for that super expensive prompt template just try to type more words and maybe you'll understand and for now I guess these prompt engineering techniques still serve as a practical work around it's just unlikely they'll be the key to the next big breakthrough you can also check out my newsletter where I covered the latest hot and spicy research that breaks down a lot of the coolest techniques that involves llms or machine learning in general and I also covered a lot of inference techniques in the previous issues which you can read for free that may also just be the next new meta in the field so don't miss out and thank you guys for watching a big shout out to Andrew lelz C SLO Dean mulim Robert zasa Owen Ingram LS Muk and many others that support me through patreon or YouTube follow me on Twitter if you haven't and I'll see you in the next one\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show transcript from a video\n",
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff947e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Summary:**\\n\\nThe video discusses the concept of prompt engineering, a technique used to optimize the performance of large language models (LLMs) by providing them with the right input to elicit the desired output. The speaker explains that prompt engineering is not a magic solution, but rather a way to tap into the capabilities that the model has already learned during its pre-training stage. The speaker also discusses the concept of Chain of Thought, a method that involves having the model articulate intermediate reasoning steps when solving complex problems. The video also touches on the idea of test-time compute, a new concept that may be the next big breakthrough in LLMs.\\n\\n**Five Key Points:**\\n\\n1. **Prompt Engineering is Not a Magic Solution**: Prompt engineering is not a way to boost a model\\'s performance, but rather a way to tap into the capabilities that the model has already learned during its pre-training stage.\\n\\n2. **Chain of Thought is Not a New Concept**: Chain of Thought is a method that involves having the model articulate intermediate reasoning steps when solving complex problems, but it\\'s not a new concept and has been around for a while.\\n\\n3. **Special Tokens are Key to Effective Chain of Thought**: The use of special tokens, such as \"end thinking\", is crucial in implementing Chain of Thought in LLMs, as it allows the model to generate more consistently and effectively.\\n\\n4. **Test-Time Compute May Be the Next Big Breakthrough**: The concept of test-time compute may be the next big breakthrough in LLMs, as it involves delaying the generation of the final answer and allowing the model to generate more tokens, which can lead to better performance.\\n\\n5. **Prompt Engineering May Be an Illusion**: The speaker suggests that prompt engineering methods may be an illusion, and that the results may be due to coincidence or the model\\'s ability to generate tokens that contain enough information to predict the next one or two tokens.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 387, 'prompt_tokens': 3315, 'total_tokens': 3702}, 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None} id='run-4ede1a91-af5d-4df8-adc6-d225aea4ef34-0' usage_metadata={'input_tokens': 3315, 'output_tokens': 387, 'total_tokens': 3702, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# using static approach to pass a prompt with a transcript of a video to LLM\n",
    "# define \"system\" and \"human\" messages for LLM as list of tuples\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        Read through the entire transcript carefully.\n",
    "        Provide a concise summary of the video's main topic and purpose.\n",
    "        Extract and list the five most interesting or important points from the transcript.\n",
    "        For each point: State the key idea in a clear and concise manner.\n",
    "\n",
    "        - Ensure your summary and key points capture the essence of the video without including unnecessary details.\n",
    "        - Use clear, engaging language that is accessible to a general audience.\n",
    "        - If the transcript includes any statistical data, expert opinions, or unique insights, prioritize including these in your summary or key points.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\"human\", data[0].page_content)\n",
    "]\n",
    "\n",
    "llm_msg = llm.invoke(messages)\n",
    "print(llm_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c7cb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dynamic approach with LLMChain for constructing a prompt with a template\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "\n",
    "# spicify summarizer_template\n",
    "summarizer_template = PromptTemplate(\n",
    "    input_variables=[\"video_transcript\"],\n",
    "    template=\"\"\"\n",
    "    Read through the entire transcript carefully.\n",
    "    Provide a concise summary of the video's main topic and purpose.\n",
    "    Extract and list the five most interesting or important points from the transcript.\n",
    "    For each point: State the key idea in a clear and concise manner.\n",
    "\n",
    "    - Ensure your summary and key points capture the essence of the video without including unnecessary details.\n",
    "    - Use clear, engaging language that is accessible to a general audience.\n",
    "    - If the transcript includes any statistical data, expert opinions, or unique insights, prioritize including these in your summary or key points.\n",
    "    \n",
    "    Video transcript: {video_transcript}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3783029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/sw6vhpc10411wn4lc23xd4t80000gn/T/ipykernel_88725/4106975217.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=summarizer_template)\n"
     ]
    }
   ],
   "source": [
    "# invoke the chain with the transcript\n",
    "chain = LLMChain(llm=llm, prompt=summarizer_template)\n",
    "\n",
    "summary = chain.invoke({\n",
    "    \"video_transcript\": data[0].page_content\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c422798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary:**\n",
       "\n",
       "The video discusses the concept of prompt engineering, a technique used to optimize the performance of large language models (LLMs) by providing them with specific instructions or examples to improve their understanding and generation of text. The speaker explains that prompt engineering is not a magic solution, but rather a way to tap into the capabilities that the model has already learned during its pre-training stage. The video also touches on the concept of Chain of Thought, a method that allows the model to articulate intermediate reasoning steps when solving complex problems, and how it can be implemented in different ways, such as through special tokens or human-augmented synthetic data.\n",
       "\n",
       "**Five Key Points:**\n",
       "\n",
       "1. **Prompt Engineering is not a magic solution**: Prompt engineering is not a way to boost a model's performance, but rather a way to tap into its existing capabilities by providing it with specific instructions or examples.\n",
       "\n",
       "2. **Chain of Thought is not a single method**: Chain of Thought is a concept that can be implemented in different ways, such as through special tokens or human-augmented synthetic data, and its effectiveness depends on how it is implemented.\n",
       "\n",
       "3. **LLMs are sensitive to prompt variations**: Large language models are highly sensitive to prompt variations, such as word order, spacing, and capitalization, which can drastically impact their performance.\n",
       "\n",
       "4. **Self-reflection and self-verification can improve performance**: Self-reflection and self-verification, which allow the model to iteratively check its answers, can significantly improve its performance on reasoning and generation tasks.\n",
       "\n",
       "5. **Prompt engineering may be an illusion**: The speaker suggests that prompt engineering methods may be an illusion, and that the model's performance improvements may be due to coincidence or the model's ability to generate more tokens, rather than the specific method used."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(summary[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413f40b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
